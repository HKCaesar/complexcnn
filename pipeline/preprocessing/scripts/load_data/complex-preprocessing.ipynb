{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "#path stuff:\n",
    "import sys\n",
    "sys.path.append('/Users/rileyedmunds/echonet/echonet')\n",
    "\n",
    "from echonet.datasets.dataset import Dataset\n",
    "from echonet.utils.generics import load_audio, to_one_hot\n",
    "\n",
    "\n",
    "class OriginalESC(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, work_dir, train_folds, validation_folds, test_folds, esc10=False):\n",
    "        super().__init__(data_dir, work_dir)\n",
    "\n",
    "        self.meta = pd.read_csv(data_dir + 'esc50.csv')\n",
    "\n",
    "        self.train_folds = train_folds\n",
    "        self.validation_folds = validation_folds\n",
    "        self.test_folds = test_folds\n",
    "\n",
    "        self.class_count = 50\n",
    "\n",
    "        self.bands = 60\n",
    "        self.segment_length = 101\n",
    "\n",
    "        self.esc10 = esc10\n",
    "        if self.esc10:\n",
    "            self.class_count = 10\n",
    "            self.meta = self.meta[self.meta['esc10']]\n",
    "            self.categories = pd.unique(self.meta.sort_values('target')['category'])\n",
    "            self.meta['target'] = self.to_targets(self.meta['category'])\n",
    "        else:\n",
    "            self.categories = pd.unique(self.meta.sort_values('target')['category'])\n",
    "\n",
    "        self.train_meta = self.meta[self.meta['fold'].isin(self.train_folds)]\n",
    "        self.validation_data.meta = self.meta[self.meta['fold'].isin(self.validation_folds)]\n",
    "        self.test_data.meta = self.meta[self.meta['fold'].isin(self.test_folds)]\n",
    "\n",
    "        self._validation_size = len(self.validation_data.meta)\n",
    "        self._test_size = len(self.test_data.meta)\n",
    "\n",
    "        self._generate_spectrograms()\n",
    "        print(\"generated spectrograms\")\n",
    "        self._populate(self.validation_data)\n",
    "        print(\"populated validation data\")\n",
    "        self._populate(self.test_data)\n",
    "        print(\"populated test data\")\n",
    "        # self._populate_portion(self.test_data)\n",
    "\n",
    "\n",
    "\n",
    "        #attempt to populate training data:\n",
    "        \n",
    "    def _generate_spectrograms(self):\n",
    "        for row in tqdm(self.meta.itertuples(), total=len(self.meta)):\n",
    "            specfile = self.work_dir + row.filename + '.orig.spec.npy'.format(self.bands)\n",
    "\n",
    "#             if os.path.exists(specfile):\n",
    "#                 continue\n",
    "\n",
    "#             print(\"go..\")\n",
    "            audio = load_audio(self.data_dir + 'audio/' + row.filename, 22050)\n",
    "            audio *= 1.0 / np.max(np.abs(audio))\n",
    "\n",
    "            #real    \n",
    "#             spec = librosa.feature.melspectrogram(audio, sr=22050, n_fft=1024,\n",
    "#                                                   hop_length=512, n_mels=self.bands)\n",
    "#             spec = librosa.logamplitude(spec)\n",
    "#             print(spec)\n",
    "\n",
    "            #complex\n",
    "            spec = librosa.core.stft(audio, n_fft=1024, hop_length=512, dtype=np.complex64)\n",
    "                #n_fft 1024 before, now 118 so that we have 60 bands\n",
    "            \n",
    "            # save_graph(spec)\n",
    "\n",
    "\n",
    "            np.save(specfile, spec, allow_pickle=False)\n",
    "\n",
    "    def _populate(self, data):\n",
    "        X, y, meta = [], [], []\n",
    "\n",
    "        for row in data.meta.itertuples():\n",
    "            segments = self._extract_all_segments(row.filename) #generates deltas and gets data out of npy to pandas dataframe\n",
    "            print(X)\n",
    "            X.extend(segments)\n",
    "            y.extend(np.repeat(row.target, len(segments)))\n",
    "            values = dict(zip(row._fields[1:], row[1:]))\n",
    "            columns = row._fields[1:]\n",
    "            rows = [pd.DataFrame(values, columns=columns, index=[0]) for _ in range(len(segments))]\n",
    "            meta.extend(rows)\n",
    "\n",
    "        X = np.stack(X)\n",
    "        y = to_one_hot(np.array(y), self.class_count)\n",
    "        meta = pd.concat(meta, ignore_index=True)\n",
    "\n",
    "        if self.data_mean is None:\n",
    "            self.data_mean = np.mean(X)\n",
    "            self.data_std = np.std(X)\n",
    "\n",
    "        X -= self.data_mean\n",
    "        X /= self.data_std\n",
    "\n",
    "        data.X = X\n",
    "        data.y = y\n",
    "        data.meta = meta\n",
    "\n",
    "\n",
    "    def _extract_all_segments(self, filename):\n",
    "        spec = np.load(self.work_dir + filename + '.orig.spec.npy')\n",
    "\n",
    "        segments = []\n",
    "        hop_length = self.segment_length // 5\n",
    "        offset = 0\n",
    "\n",
    "        while offset < np.shape(spec)[1] - self.segment_length:\n",
    "            segment = spec[:, offset:offset + self.segment_length]\n",
    "            delta = self._generate_delta(segment)\n",
    "            offset += hop_length\n",
    "            segments.append(np.stack([segment, delta])) #segment.real\n",
    "\n",
    "        return segments\n",
    "\n",
    "\n",
    "    def _extract_segment(self, filename):\n",
    "        spec = np.load(self.work_dir + filename + '.orig.spec.npy')\n",
    "        offset = self.RandomState.randint(0, np.shape(spec)[1] - self.segment_length + 1)\n",
    "        spec = spec[:, offset:offset + self.segment_length]\n",
    "        delta = self._generate_delta(spec)\n",
    "        return np.stack([spec, delta])\n",
    "\n",
    "    def _generate_delta(self, spec):\n",
    "        # ported librosa v0.3.1. implementation\n",
    "        window = np.arange(4, -5, -1)\n",
    "        padding = [(0, 0), (5, 5)]\n",
    "        delta = np.pad(spec, padding, mode='edge')\n",
    "        delta = scipy.signal.lfilter(window, 1, delta, axis=-1)\n",
    "        idx = [Ellipsis, slice(5, -5, None)]\n",
    "        return delta[idx]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_shape(self):\n",
    "        return 2, self.bands, self.segment_length\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        return len(self.train_meta)\n",
    "\n",
    "    @property\n",
    "    def validation_size(self):\n",
    "        return self._validation_size\n",
    "\n",
    "    @property\n",
    "    def validation_segments(self):\n",
    "        return len(self.validation_data.meta)\n",
    "\n",
    "    @property\n",
    "    def test_size(self):\n",
    "        return self._test_size\n",
    "\n",
    "    @property\n",
    "    def test_segments(self):\n",
    "        return len(self.test_data.meta)\n",
    "\n",
    "    def to_categories(self, targets):\n",
    "        return self.categories[targets]\n",
    "\n",
    "    def to_targets(self, categories):\n",
    "        return [np.argmax(self.categories == name) for name in categories]\n",
    "\n",
    "    def test(self, model):\n",
    "        return self._score(model, self.test_data)\n",
    "\n",
    "    def validate(self, model):\n",
    "        return self._score(model, self.validation_data)\n",
    "\n",
    "    def iterbatches(self, batch_size):\n",
    "        itrain = super()._iterrows(self.train_meta)\n",
    "\n",
    "        while True:\n",
    "            X, y = [], []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                row = next(itrain)\n",
    "                X.append(self._extract_segment(row.filename))\n",
    "                y.append(row.target)\n",
    "\n",
    "            X = np.stack(X)\n",
    "            y = to_one_hot(np.array(y), self.class_count)\n",
    "\n",
    "            X -= self.data_mean\n",
    "            X /= self.data_std\n",
    "\n",
    "            yield X, y\n",
    "\n",
    "\n",
    "\n",
    "    #beyond here is for training, and not used for data creation-----------\n",
    "\n",
    "    def _score(self, model, data):\n",
    "        predictions = pd.DataFrame(model.predict(data.X))\n",
    "        results = pd.concat([data.meta[['filename', 'target']], predictions], axis=1)\n",
    "        results = results.groupby('filename').aggregate('mean').reset_index()\n",
    "        results['predicted'] = np.argmax(results.iloc[:, 2:].values, axis=1)\n",
    "        return np.sum(results['predicted'] == results['target']) / len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####REAL\n",
    "\n",
    "\n",
    "# Paper source code ported to Keras with some small adjustments.\n",
    "\n",
    "# Reference:\n",
    "\n",
    "# - [Environmental Sound Classification with Convolutional Neural Networks -\n",
    "#     paper replication data](https://github.com/karoldvl/paper-2015-esc-convnet)\n",
    "\n",
    "\n",
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "#path stuff:\n",
    "import sys\n",
    "sys.path.append('/Users/rileyedmunds/echonet/echonet')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--device', help='Theano device used for computations')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "RANDOM_SEED = 20161013\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# DEVICE = args.device if args.device else 'gpu0'\n",
    "THEANO_FLAGS = ('device={},'\n",
    "                'floatX=float32,'\n",
    "                'dnn.conv.algo_bwd_filter=deterministic,'\n",
    "                'dnn.conv.algo_bwd_data=deterministic').format('gpu0')\n",
    "os.environ['THEANO_FLAGS'] = THEANO_FLAGS\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.dirname(__file__)) + '/..')\n",
    "# sys.path.append(os.path.abspath(os.path.dirname('/Users/rileyedmunds/echonet/echonet/examples')) + '/..')\n",
    "\n",
    "import keras\n",
    "# keras.backend.set_image_dim_ordering('th')\n",
    "# from keras.layers.convolutional import Convolution2D as Conv\n",
    "# from keras.layers.convolutional import MaxPooling2D as Pool\n",
    "# from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "\n",
    "from echonet.models import EchoNet\n",
    "# from echonet.datasets.esc_original_real import OriginalESC as OriginalESC_real\n",
    "# from echonet.datasets.esc_original import OriginalESC as OriginalESC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def uniform(scale):\n",
    "#     return functools.partial(keras.initializations.uniform, scale=scale)\n",
    "\n",
    "# def normal(stdev):\n",
    "#     return functools.partial(keras.initializations.normal, scale=stdev)\n",
    "\n",
    "# TRAIN_FOLDS = [2, 3, 4]\n",
    "    # VALIDATION_FOLDS = [5]\n",
    "    # TEST_FOLDS = [1]\n",
    "\n",
    "    #Note using validation as training and test as test.\n",
    "TRAIN_FOLDS = [2, 3, 4]\n",
    "VALIDATION_FOLDS = [2, 3, 4]\n",
    "TEST_FOLDS = [1]\n",
    "\n",
    "print('\\nLoading ESC-50 dataset from ../data/ESC-50/')\n",
    "# esc50_real = OriginalESC_real('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "#                     TEST_FOLDS)\n",
    "esc50 = OriginalESC('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "                    TEST_FOLDS)\n",
    "# esc50_real = OriginalESC_real('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "#                     TEST_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#populate training from validation\n",
    "\n",
    "esc50.train_data = esc50.validation_data\n",
    "esc50.train_folds = esc50.validation_folds\n",
    "esc50.train_segments = esc50.validation_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "#save out test data:\n",
    "with h5py.File('testh5pyreal.h5', 'w') as test:\n",
    "    test.create_dataset('label', data=np.array(esc50.test_data.meta['target']))\n",
    "    test.create_dataset('data', data=esc50.test_data.X)\n",
    "    \n",
    "#save out train data:\n",
    "with h5py.File('trainh5pyreal.h5', 'w') as train:\n",
    "    train.create_dataset('label', data=np.array(esc50.train_data.meta['target']))\n",
    "    train.create_dataset('data', data=esc50.train_data.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esc50.test_data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50.test_data.X.shape\n",
    "# esc50.train\n",
    "from pprint import pprint\n",
    "pprint(vars(esc50))\n",
    "# X -= self.data_mean\n",
    "#             X /= self.data_std\n",
    "# esc50.test_data.X.dtype\n",
    "# esc50_real.test_data.X[0][0][0][100]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten data\n",
    "# train = {'data': esc50.train_data.X, 'label': np.array(esc50.train_data.meta['target'])}\n",
    "# test = {'data': esc50.test_data.X, 'label': np.array(esc50.test_data.meta['target'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train['label'].shape\n",
    "# type(train['label'])\n",
    "# n = np.array(esc50.test_data.meta['target'])\n",
    "# np.array(train['label']['target'])\n",
    "# train['data'].shape\n",
    "# test['label']\n",
    "print(esc50.test_data.meta['target'].shape)\n",
    "print(esc50.train_data.meta['target'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "#save out test data:\n",
    "# with h5py.File('testh5py.h5', 'w') as test:\n",
    "#     test.create_dataset('label', np.array(esc50.test_data.meta['target']))\n",
    "#     test.create_dataset('data', data=esc50.test_data.X)\n",
    "\n",
    "#NOT WORKING DIMENSIONALITY ISSUE ON H5PY SAVE!\n",
    "\n",
    "\n",
    "\n",
    "#save out test data:\n",
    "with h5py.File('testh5py.h5', 'w') as test:\n",
    "    test.create_dataset('label', data=np.array(esc50.test_data.meta['target']))\n",
    "    test.create_dataset('data', data=esc50.test_data.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#save out train data:\n",
    "with h5py.File('trainh5py.h5', 'w') as train:\n",
    "    train.create_dataset('label', data=np.array(esc50.train_data.meta['target']))\n",
    "    train.create_dataset('data', data=esc50.train_data.X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save train and test individually\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "dd.io.save('train.h5', train, compression=None)\n",
    "dd.io.save('test.h5', test, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "\n",
    "# with h5py.File('data.h5', 'w') as hf:\n",
    "#     hf.create_dataset('data', data=train['data'])\n",
    "#     hf.create_dataset('label', data=train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esc50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# # pprint(vars(esc50))\n",
    "# # pprint(vars(esc50))\n",
    "# # pprint(vars(esc50_real))\n",
    "\n",
    "# # esc50 = []\n",
    "# # pprint(vars(esc50.test_data))\n",
    "# # esc50.train_folds\n",
    "# print(esc50.train_size)\n",
    "# print(esc50.test_size)\n",
    "# print(esc50.validation_size)\n",
    "# print(esc50.test_data.X.shape)\n",
    "# print(esc50.validation_data.X.shape)\n",
    "# print(esc50_real.test_data.X.shape)\n",
    "# print(esc50_real.validation_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(esc50.__dict__)\n",
    "# print(type(esc50))\n",
    "# for key, value in esc50.items() :\n",
    "#     print (key)\n",
    "# print(esc50.test_data)\n",
    "# print(esc50.test_size)\n",
    "# print(esc50.train_data)\n",
    "# print(esc50.train_size)\n",
    "\n",
    "\n",
    "# print(esc50.train_data.meta.shape)\n",
    "# print(esc50.train_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50.train_data\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import hdf5storage as hdf5storage\n",
    "\n",
    "# hdf5storage.writes(esc50, filename='newdata.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import deepdish as dd\n",
    "# import numpy as np\n",
    "\n",
    "# X = np.zeros((100, 3, 32, 32))\n",
    "# y = np.zeros(100)\n",
    "\n",
    "# dd.io.save('data.h5', esc50, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esc50train = esc50.train_data\n",
    "# esc50test = esc50.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = {'data': esc50.train_data.X, 'label': esc50.train_data.meta}\n",
    "# test = {'data': esc50.test_data.X, 'label': esc50.test_data.meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50test.X.shape\n",
    "# esc50test.meta.shape\n",
    "\n",
    "# #save train and test individually\n",
    "# dd.io.save('esc50train.h5', esc50train, compression=None)\n",
    "# dd.io.save('esc50test.h5', esc50test, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50 == esc50_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #python dictionary checker\n",
    "# def find(lst, key, value):\n",
    "#     for i, dic in enumerate(lst):\n",
    "#         if dic[key] == value:\n",
    "#             return i\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # esc50.test_data\n",
    "# print(esc50.test_data.X.dtype)\n",
    "# print(esc50.test_data.X.shape)\n",
    "\n",
    "# print(esc50.test_data.X.dtype)\n",
    "# print(esc50_real.test_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATTEMPTS AT SHOWING A SPECTROGRAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import librosa\n",
    "# plt.figure(figsize=(12,4))\n",
    "# #path stuff:\n",
    "# import sys\n",
    "# sys.path.append('/Users/rileyedmunds/echonet/echonet')\n",
    "# # %load '../echonet'\n",
    "# # %load '../echonet/datasets/esc_original.py'\n",
    "\n",
    "\n",
    "# specfile = esc50.work_dir + '5-9032-A-0.wav' + '.orig.spec.npy'.format(esc50.bands)\n",
    "\n",
    "# audio = echonet.load_audio(self.data_dir + 'audio/' + row.filename, 22050)\n",
    "# audio *= 1.0 / np.max(np.abs(audio))\n",
    "\n",
    "# spec = librosa.feature.melspectrogram(audio, sr=22050, n_fft=1024,\n",
    "#                                       hop_length=512, n_mels=self.bands)\n",
    "# spec = librosa.logamplitude(spec)\n",
    "\n",
    "# np.save(specfile, spec, allow_pickle=False)\n",
    "\n",
    "\n",
    "\n",
    "# # Display the spectrogram on a mel scale\n",
    "# # sample rate and hop length parameters are used to render the time axis\n",
    "# librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
    "\n",
    "# # Put a descriptive title on the plot\n",
    "# plt.title('mel power spectrogram')\n",
    "\n",
    "# # draw a color bar\n",
    "# plt.colorbar(format='%+02.0f dB')\n",
    "\n",
    "\n",
    "# # Make the figure layout compact\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
