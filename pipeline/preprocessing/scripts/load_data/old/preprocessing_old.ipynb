{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.2.0 (64-bit)\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "#path stuff:\n",
    "import sys\n",
    "sys.path.append('/u/vis/x1/renswny/complexsound/echonet')\n",
    "\n",
    "from echonet.datasets.dataset import Dataset\n",
    "from echonet.utils.generics import load_audio, to_one_hot\n",
    "\n",
    "\n",
    "class OriginalESC(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, work_dir, train_folds, validation_folds, test_folds, esc10=False):\n",
    "        super().__init__(data_dir, work_dir)\n",
    "\n",
    "        self.meta = pd.read_csv(data_dir + 'esc50.csv')\n",
    "\n",
    "        self.train_folds = train_folds\n",
    "        self.validation_folds = validation_folds\n",
    "        self.test_folds = test_folds\n",
    "\n",
    "        self.class_count = 50\n",
    "\n",
    "        self.bands = 60\n",
    "        self.segment_length = 101\n",
    "\n",
    "        self.esc10 = esc10\n",
    "        if self.esc10:\n",
    "            self.class_count = 10\n",
    "            self.meta = self.meta[self.meta['esc10']]\n",
    "            self.categories = pd.unique(self.meta.sort_values('target')['category'])\n",
    "            self.meta['target'] = self.to_targets(self.meta['category'])\n",
    "        else:\n",
    "            self.categories = pd.unique(self.meta.sort_values('target')['category'])\n",
    "\n",
    "        self.train_meta = self.meta[self.meta['fold'].isin(self.train_folds)]\n",
    "        self.validation_data.meta = self.meta[self.meta['fold'].isin(self.validation_folds)]\n",
    "        self.test_data.meta = self.meta[self.meta['fold'].isin(self.test_folds)]\n",
    "\n",
    "        self._validation_size = len(self.validation_data.meta)\n",
    "        self._test_size = len(self.test_data.meta)\n",
    "\n",
    "        self._generate_spectrograms()\n",
    "        print(\"generated spectrograms\")\n",
    "        self._populate(self.validation_data)\n",
    "        print(\"populated validation data\")\n",
    "        self._populate(self.test_data)\n",
    "        print(\"populated test data\")\n",
    "        # self._populate_portion(self.test_data)\n",
    "\n",
    "\n",
    "\n",
    "        #attempt to populate training data:\n",
    "        \n",
    "    def _generate_spectrograms(self):\n",
    "        for row in tqdm(self.meta.itertuples(), total=len(self.meta)):\n",
    "            specfile = self.work_dir + row.filename + '.orig.spec.npy'.format(self.bands)\n",
    "\n",
    "#             if os.path.exists(specfile):\n",
    "#                 continue\n",
    "\n",
    "#             print(\"go..\")\n",
    "            audio = load_audio(self.data_dir + 'audio/' + row.filename, 22050)\n",
    "            audio *= 1.0 / np.max(np.abs(audio))\n",
    "\n",
    "            #real    \n",
    "            spec = librosa.feature.melspectrogram(audio, sr=22050, n_fft=1024,\n",
    "                                                  hop_length=512, n_mels=self.bands)\n",
    "            spec = librosa.logamplitude(spec)\n",
    "#             print(spec)\n",
    "\n",
    "            #complex\n",
    "#             spec = librosa.core.stft(audio, n_fft=1024, hop_length=512, dtype=np.complex64)\n",
    "\n",
    "            # save_graph(spec)\n",
    "\n",
    "\n",
    "            np.save(specfile, spec, allow_pickle=False)\n",
    "\n",
    "    def _populate(self, data):\n",
    "        X, y, meta = [], [], []\n",
    "\n",
    "        for row in data.meta.itertuples():\n",
    "            segments = self._extract_all_segments(row.filename) #generates deltas and gets data out of npy to pandas dataframe\n",
    "            X.extend(segments)\n",
    "            y.extend(np.repeat(row.target, len(segments)))\n",
    "            values = dict(zip(row._fields[1:], row[1:]))\n",
    "            columns = row._fields[1:]\n",
    "            rows = [pd.DataFrame(values, columns=columns, index=[0]) for _ in range(len(segments))]\n",
    "            meta.extend(rows)\n",
    "\n",
    "        X = np.stack(X)\n",
    "        y = to_one_hot(np.array(y), self.class_count)\n",
    "        meta = pd.concat(meta, ignore_index=True)\n",
    "\n",
    "        if self.data_mean is None:\n",
    "            self.data_mean = np.mean(X)\n",
    "            self.data_std = np.std(X)\n",
    "\n",
    "        X -= self.data_mean\n",
    "        X /= self.data_std\n",
    "\n",
    "        data.X = X\n",
    "        data.y = y\n",
    "        data.meta = meta\n",
    "\n",
    "\n",
    "    def _extract_all_segments(self, filename):\n",
    "        spec = np.load(self.work_dir + filename + '.orig.spec.npy')\n",
    "\n",
    "        segments = []\n",
    "        hop_length = self.segment_length // 5\n",
    "        offset = 0\n",
    "\n",
    "        while offset < np.shape(spec)[1] - self.segment_length:\n",
    "            segment = spec[:, offset:offset + self.segment_length]\n",
    "            delta = self._generate_delta(segment)\n",
    "            offset += hop_length\n",
    "            segments.append(np.stack([segment, delta]))\n",
    "\n",
    "        return segments\n",
    "\n",
    "\n",
    "    def _extract_segment(self, filename):\n",
    "        spec = np.load(self.work_dir + filename + '.orig.spec.npy')\n",
    "        offset = self.RandomState.randint(0, np.shape(spec)[1] - self.segment_length + 1)\n",
    "        spec = spec[:, offset:offset + self.segment_length]\n",
    "        delta = self._generate_delta(spec)\n",
    "        return np.stack([spec, delta])\n",
    "\n",
    "    def _generate_delta(self, spec):\n",
    "        # ported librosa v0.3.1. implementation\n",
    "        window = np.arange(4, -5, -1)\n",
    "        padding = [(0, 0), (5, 5)]\n",
    "        delta = np.pad(spec, padding, mode='edge')\n",
    "        delta = scipy.signal.lfilter(window, 1, delta, axis=-1)\n",
    "        idx = [Ellipsis, slice(5, -5, None)]\n",
    "        return delta[idx]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_shape(self):\n",
    "        return 2, self.bands, self.segment_length\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        return len(self.train_meta)\n",
    "\n",
    "    @property\n",
    "    def validation_size(self):\n",
    "        return self._validation_size\n",
    "\n",
    "    @property\n",
    "    def validation_segments(self):\n",
    "        return len(self.validation_data.meta)\n",
    "\n",
    "    @property\n",
    "    def test_size(self):\n",
    "        return self._test_size\n",
    "\n",
    "    @property\n",
    "    def test_segments(self):\n",
    "        return len(self.test_data.meta)\n",
    "\n",
    "    def to_categories(self, targets):\n",
    "        return self.categories[targets]\n",
    "\n",
    "    def to_targets(self, categories):\n",
    "        return [np.argmax(self.categories == name) for name in categories]\n",
    "\n",
    "    def test(self, model):\n",
    "        return self._score(model, self.test_data)\n",
    "\n",
    "    def validate(self, model):\n",
    "        return self._score(model, self.validation_data)\n",
    "\n",
    "    def iterbatches(self, batch_size):\n",
    "        itrain = super()._iterrows(self.train_meta)\n",
    "\n",
    "        while True:\n",
    "            X, y = [], []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                row = next(itrain)\n",
    "                X.append(self._extract_segment(row.filename))\n",
    "                y.append(row.target)\n",
    "\n",
    "            X = np.stack(X)\n",
    "            y = to_one_hot(np.array(y), self.class_count)\n",
    "\n",
    "            X -= self.data_mean\n",
    "            X /= self.data_std\n",
    "\n",
    "            yield X, y\n",
    "\n",
    "\n",
    "\n",
    "    #beyond here is for training, and not used for data creation-----------\n",
    "\n",
    "    def _score(self, model, data):\n",
    "        predictions = pd.DataFrame(model.predict(data.X))\n",
    "        results = pd.concat([data.meta[['filename', 'target']], predictions], axis=1)\n",
    "        results = results.groupby('filename').aggregate('mean').reset_index()\n",
    "        results['predicted'] = np.argmax(results.iloc[:, 2:].values, axis=1)\n",
    "        return np.sum(results['predicted'] == results['target']) / len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/u/vis/x1/redmunds/anaconda3.4/lib/python35.zip', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/plat-linux', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/lib-dynload', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/site-packages', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/site-packages/Sphinx-1.4.6-py3.5.egg', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/site-packages/setuptools-27.2.0-py3.5.egg', '/u/vis/x1/redmunds/anaconda3.4/lib/python3.5/site-packages/IPython/extensions', '/n/shokuji/dd/renswny/.ipython', '/u/vis/x1/renswny/complexsound/echonet/echonet', '/u/vis/x1/renswny/complexsound/echonet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu0 is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ESC-50 dataset from ../data/ESC-50/\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "/n/whiskey/xy/vis/renswny/complexsound/echonet/data/ESC-50/ is not a proper dataset directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cc5405f383ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m#                     TEST_FOLDS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m esc50 = OriginalESC('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n\u001b[0;32m---> 77\u001b[0;31m                     TEST_FOLDS)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m# esc50_real = OriginalESC_real('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#                     TEST_FOLDS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-08bf60b93fc7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, work_dir, train_folds, validation_folds, test_folds, esc10)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesc10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'esc50.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/vis/x1/renswny/complexsound/echonet/echonet/datasets/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, work_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is not a proper dataset directory.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is not a proper working directory.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: /n/whiskey/xy/vis/renswny/complexsound/echonet/data/ESC-50/ is not a proper dataset directory."
     ]
    }
   ],
   "source": [
    "####REAL\n",
    "\n",
    "\n",
    "# Paper source code ported to Keras with some small adjustments.\n",
    "\n",
    "# Reference:\n",
    "\n",
    "# - [Environmental Sound Classification with Convolutional Neural Networks -\n",
    "#     paper replication data](https://github.com/karoldvl/paper-2015-esc-convnet)\n",
    "\n",
    "\n",
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "#path stuff:\n",
    "import sys\n",
    "sys.path.append('/u/vis/x1/renswny/complexsound/echonet')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--device', help='Theano device used for computations')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "RANDOM_SEED = 20161013\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# DEVICE = args.device if args.device else 'gpu0'\n",
    "THEANO_FLAGS = ('device={},'\n",
    "                'floatX=float32,'\n",
    "                'dnn.conv.algo_bwd_filter=deterministic,'\n",
    "                'dnn.conv.algo_bwd_data=deterministic').format('gpu0')\n",
    "os.environ['THEANO_FLAGS'] = THEANO_FLAGS\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.dirname(__file__)) + '/..')\n",
    "# sys.path.append(os.path.abspath(os.path.dirname('/Users/rileyedmunds/echonet/echonet/examples')) + '/..')\n",
    "\n",
    "import keras\n",
    "# keras.backend.set_image_dim_ordering('th')\n",
    "# from keras.layers.convolutional import Convolution2D as Conv\n",
    "# from keras.layers.convolutional import MaxPooling2D as Pool\n",
    "# from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "\n",
    "from echonet.models import EchoNet\n",
    "# from echonet.datasets.esc_original_real import OriginalESC as OriginalESC_real\n",
    "# from echonet.datasets.esc_original import OriginalESC as OriginalESC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def uniform(scale):\n",
    "#     return functools.partial(keras.initializations.uniform, scale=scale)\n",
    "\n",
    "# def normal(stdev):\n",
    "#     return functools.partial(keras.initializations.normal, scale=stdev)\n",
    "\n",
    "# TRAIN_FOLDS = [2, 3, 4]\n",
    "    # VALIDATION_FOLDS = [5]\n",
    "    # TEST_FOLDS = [1]\n",
    "\n",
    "    #Note using validation as training and test as test.\n",
    "TRAIN_FOLDS = [2, 3, 4]\n",
    "VALIDATION_FOLDS = [2, 3, 4]\n",
    "TEST_FOLDS = [1]\n",
    "\n",
    "print('\\nLoading ESC-50 dataset from ../data/ESC-50/')\n",
    "# esc50_real = OriginalESC_real('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "#                     TEST_FOLDS)\n",
    "esc50 = OriginalESC('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "                    TEST_FOLDS)\n",
    "# esc50_real = OriginalESC_real('../data/ESC-50/', '../data/.ESC-50.cache', TRAIN_FOLDS, VALIDATION_FOLDS,\n",
    "#                     TEST_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#populate training from validation\n",
    "\n",
    "esc50.train_data = esc50.validation_data\n",
    "esc50.train_folds = esc50.validation_folds\n",
    "esc50.train_segments = esc50.validation_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "#save out test data:\n",
    "with h5py.File('testh5pyreal.h5', 'w') as test:\n",
    "    test.create_dataset('label', data=np.array(esc50.test_data.meta['target']))\n",
    "    test.create_dataset('data', data=esc50.test_data.X)\n",
    "    \n",
    "#save out train data:\n",
    "with h5py.File('trainh5pyreal.h5', 'w') as train:\n",
    "    train.create_dataset('label', data=np.array(esc50.train_data.meta['target']))\n",
    "    train.create_dataset('data', data=esc50.train_data.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2, 60, 101)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esc50.test_data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50.test_data.X.shape\n",
    "# esc50.train\n",
    "from pprint import pprint\n",
    "pprint(vars(esc50))\n",
    "# X -= self.data_mean\n",
    "#             X /= self.data_std\n",
    "# esc50.test_data.X.dtype\n",
    "# esc50_real.test_data.X[0][0][0][100]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten data\n",
    "# train = {'data': esc50.train_data.X, 'label': np.array(esc50.train_data.meta['target'])}\n",
    "# test = {'data': esc50.test_data.X, 'label': np.array(esc50.test_data.meta['target'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 2, 513, 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(7200,)\n"
     ]
    }
   ],
   "source": [
    "# train['label'].shape\n",
    "# type(train['label'])\n",
    "# n = np.array(esc50.test_data.meta['target'])\n",
    "# np.array(train['label']['target'])\n",
    "# train['data'].shape\n",
    "# test['label']\n",
    "print(esc50.test_data.meta['target'].shape)\n",
    "print(esc50.train_data.meta['target'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "#save out test data:\n",
    "# with h5py.File('testh5py.h5', 'w') as test:\n",
    "#     test.create_dataset('label', np.array(esc50.test_data.meta['target']))\n",
    "#     test.create_dataset('data', data=esc50.test_data.X)\n",
    "\n",
    "#NOT WORKING DIMENSIONALITY ISSUE ON H5PY SAVE!\n",
    "\n",
    "\n",
    "\n",
    "#save out test data:\n",
    "with h5py.File('testh5py.h5', 'w') as test:\n",
    "    test.create_dataset('label', data=np.array(esc50.test_data.meta['target']))\n",
    "    test.create_dataset('data', data=esc50.test_data.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#save out train data:\n",
    "with h5py.File('trainh5py.h5', 'w') as train:\n",
    "    train.create_dataset('label', data=np.array(esc50.train_data.meta['target']))\n",
    "    train.create_dataset('data', data=esc50.train_data.X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save train and test individually\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "dd.io.save('train.h5', train, compression=None)\n",
    "dd.io.save('test.h5', test, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "\n",
    "# with h5py.File('data.h5', 'w') as hf:\n",
    "#     hf.create_dataset('data', data=train['data'])\n",
    "#     hf.create_dataset('label', data=train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esc50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# # pprint(vars(esc50))\n",
    "# # pprint(vars(esc50))\n",
    "# # pprint(vars(esc50_real))\n",
    "\n",
    "# # esc50 = []\n",
    "# # pprint(vars(esc50.test_data))\n",
    "# # esc50.train_folds\n",
    "# print(esc50.train_size)\n",
    "# print(esc50.test_size)\n",
    "# print(esc50.validation_size)\n",
    "# print(esc50.test_data.X.shape)\n",
    "# print(esc50.validation_data.X.shape)\n",
    "# print(esc50_real.test_data.X.shape)\n",
    "# print(esc50_real.validation_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(esc50.__dict__)\n",
    "# print(type(esc50))\n",
    "# for key, value in esc50.items() :\n",
    "#     print (key)\n",
    "# print(esc50.test_data)\n",
    "# print(esc50.test_size)\n",
    "# print(esc50.train_data)\n",
    "# print(esc50.train_size)\n",
    "\n",
    "\n",
    "# print(esc50.train_data.meta.shape)\n",
    "# print(esc50.train_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50.train_data\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import hdf5storage as hdf5storage\n",
    "\n",
    "# hdf5storage.writes(esc50, filename='newdata.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import deepdish as dd\n",
    "# import numpy as np\n",
    "\n",
    "# X = np.zeros((100, 3, 32, 32))\n",
    "# y = np.zeros(100)\n",
    "\n",
    "# dd.io.save('data.h5', esc50, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esc50train = esc50.train_data\n",
    "# esc50test = esc50.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = {'data': esc50.train_data.X, 'label': esc50.train_data.meta}\n",
    "# test = {'data': esc50.test_data.X, 'label': esc50.test_data.meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-16751be86ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#save train and test individually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'esc50train.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesc50train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'esc50test.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesc50test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dd' is not defined"
     ]
    }
   ],
   "source": [
    "# esc50test.X.shape\n",
    "# esc50test.meta.shape\n",
    "\n",
    "# #save train and test individually\n",
    "# dd.io.save('esc50train.h5', esc50train, compression=None)\n",
    "# dd.io.save('esc50test.h5', esc50test, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# esc50 == esc50_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #python dictionary checker\n",
    "# def find(lst, key, value):\n",
    "#     for i, dic in enumerate(lst):\n",
    "#         if dic[key] == value:\n",
    "#             return i\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(2400, 2, 60, 101)\n",
      "float64\n",
      "(2400, 2, 60, 101)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # esc50.test_data\n",
    "# print(esc50.test_data.X.dtype)\n",
    "# print(esc50.test_data.X.shape)\n",
    "\n",
    "# print(esc50.test_data.X.dtype)\n",
    "# print(esc50_real.test_data.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATTEMPTS AT SHOWING A SPECTROGRAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import librosa\n",
    "# plt.figure(figsize=(12,4))\n",
    "# #path stuff:\n",
    "# import sys\n",
    "# sys.path.append('/Users/rileyedmunds/echonet/echonet')\n",
    "# # %load '../echonet'\n",
    "# # %load '../echonet/datasets/esc_original.py'\n",
    "\n",
    "\n",
    "# specfile = esc50.work_dir + '5-9032-A-0.wav' + '.orig.spec.npy'.format(esc50.bands)\n",
    "\n",
    "# audio = echonet.load_audio(self.data_dir + 'audio/' + row.filename, 22050)\n",
    "# audio *= 1.0 / np.max(np.abs(audio))\n",
    "\n",
    "# spec = librosa.feature.melspectrogram(audio, sr=22050, n_fft=1024,\n",
    "#                                       hop_length=512, n_mels=self.bands)\n",
    "# spec = librosa.logamplitude(spec)\n",
    "\n",
    "# np.save(specfile, spec, allow_pickle=False)\n",
    "\n",
    "\n",
    "\n",
    "# # Display the spectrogram on a mel scale\n",
    "# # sample rate and hop length parameters are used to render the time axis\n",
    "# librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
    "\n",
    "# # Put a descriptive title on the plot\n",
    "# plt.title('mel power spectrogram')\n",
    "\n",
    "# # draw a color bar\n",
    "# plt.colorbar(format='%+02.0f dB')\n",
    "\n",
    "\n",
    "# # Make the figure layout compact\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
